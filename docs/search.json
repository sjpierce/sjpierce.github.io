[
  {
    "objectID": "rr_software.html",
    "href": "rr_software.html",
    "title": "Software for Reproducible Research with R",
    "section": "",
    "text": "Statisticians can demonstrate leadership in research projects by advocating for and engaging in reproducible research (Hochheimer et al., 2024). Reproducible research can be facilitated by using a set of software packages that combine into a powerful, integrated set of tools. The statistical software R, paired with RStudio Desktop (RStudio Team, 2026), is a foundation for a set of tools that can generate dynamic documents containing a mix of narrative text along with R code that can be compiled to automatically produce a fully-formatted report, manuscript, website, or set of presentation slides complete with text, equations, figures, tables, and reference sections (Mair, 2016).\nWhile Mair (2016) introduces R Markdown as a tool for writing dynamic documents, Quarto (Allaire et al., 2025) is a next-generation tool that replaces R Markdown. Quarto is a key part of scientific publishing for those who want to do reproducible research. It can produce documents in many different formats (e.g., HTML, PDF, Word, PowerPoint, and many more). Combining Quarto with TinyTeX enables production of very nice PDF files, which are great for distribution and archiving. Meanwhile, Git (Torvalds et al., 2025) and GitHub solve source code version control problems and facilitate collaboration on code (Bryan, 2018).\nMarwick et al. (2018) discuss creating research compendiums that bundle collections of documents along with data files into custom R packages to make a research project more reproducible. These software packages become additional scholarly products that make manuscripts more reproducible."
  },
  {
    "objectID": "rr_software.html#r-for-windows",
    "href": "rr_software.html#r-for-windows",
    "title": "Software for Reproducible Research with R",
    "section": "3.1 R for Windows",
    "text": "3.1 R for Windows\nIf you are using Windows, follow the links labeled Download R for Windows, then base, then Download R-4.5.2 for Windows or use the direct link to the Windows installer. Save that installation file to your computer, then run it."
  },
  {
    "objectID": "rr_software.html#r-for-macos",
    "href": "rr_software.html#r-for-macos",
    "title": "Software for Reproducible Research with R",
    "section": "3.2 R for MacOS",
    "text": "3.2 R for MacOS\nIf you are using MacOS, follow the link labeled Download R for macOS, then download the latest release specific to your version of MacOS and whether you have an Intel processor or ARM processor! Those come with different requirements for which version of XCode and other tools have to be installed to compile R packages."
  },
  {
    "objectID": "rr_software.html#r-installation-tips",
    "href": "rr_software.html#r-installation-tips",
    "title": "Software for Reproducible Research with R",
    "section": "3.3 R Installation Tips",
    "text": "3.3 R Installation Tips\nI recommend selecting the following options when installing R.\n\n64-bit User installation\nCustomized startup\nSDI (separate windows)\nHTML help\nSave version number in registry\nAssociate R with .RData files\n\nIf you previously were using an older version of R (any version in the 4.4.x series), you should plan to reinstall all your R packages from scratch under R 4.5.0 or later. The best way to do that is to use a script such as Reinstall_Packages.R under the older version to save a data file containing the names of installed packages, then remove the older version of R and replace it with the newest version of R, and use the remainder of that script to read in that list of packages and install them. That will take several minutes if you have a lot of packages."
  },
  {
    "objectID": "rr_software.html#rtools-for-windows-version-4.4-6414-6401-or-later",
    "href": "rr_software.html#rtools-for-windows-version-4.4-6414-6401-or-later",
    "title": "Software for Reproducible Research with R",
    "section": "4.1 RTools for Windows, version 4.4 (6414-6401 or later)",
    "text": "4.1 RTools for Windows, version 4.4 (6414-6401 or later)\nInstalling RTools is important because it contains some tools for compiling software packages and one of the R packages we will use later (the devtools package) depends on having RTools available. You can download RTools from CRAN. Either follow the CRAN links labeled Download R for Windows, then RTools, then RTools 4.4, then RTools44_installer, or use the direct link to the installer file. Save that installation file to your computer, then run it."
  },
  {
    "objectID": "rr_software.html#macos-xcode-gnu-fortran-and-mandatory-libraries",
    "href": "rr_software.html#macos-xcode-gnu-fortran-and-mandatory-libraries",
    "title": "Software for Reproducible Research with R",
    "section": "4.2 MacOS: Xcode, GNU Fortran, and Mandatory Libraries",
    "text": "4.2 MacOS: Xcode, GNU Fortran, and Mandatory Libraries\nIf you are using MacOS instead of Windows, then you need a few MacOS-specific development tools that fulfill the compiler functions that RTools serves on Windows. The details are available at https://mac.r-project.org/tools/. Specifically, you need Xcode, GNU Fortran 12.2 Compiler, and some mandatory libraries."
  },
  {
    "objectID": "rr_software.html#rstudio-for-windows",
    "href": "rr_software.html#rstudio-for-windows",
    "title": "Software for Reproducible Research with R",
    "section": "5.1 RStudio for Windows",
    "text": "5.1 RStudio for Windows\nYou can use this direct link to the Windows installer. Save that installation file to your computer’s Downloads folder, then run it."
  },
  {
    "objectID": "rr_software.html#rstudio-for-macos",
    "href": "rr_software.html#rstudio-for-macos",
    "title": "Software for Reproducible Research with R",
    "section": "5.2 RStudio for MacOS",
    "text": "5.2 RStudio for MacOS\nYou can use this direct link for the MacOS installer."
  },
  {
    "objectID": "rr_software.html#quarto-for-windows",
    "href": "rr_software.html#quarto-for-windows",
    "title": "Software for Reproducible Research with R",
    "section": "6.1 Quarto for Windows",
    "text": "6.1 Quarto for Windows\nYou can use this direct link to the Windows installer. Save that installation file to your computer’s Downloads folder, then run it."
  },
  {
    "objectID": "rr_software.html#quarto-for-macos",
    "href": "rr_software.html#quarto-for-macos",
    "title": "Software for Reproducible Research with R",
    "section": "6.2 Quarto for MacOS",
    "text": "6.2 Quarto for MacOS\nYou can use this direct link to the MacOS installer."
  },
  {
    "objectID": "rr_software.html#git-for-windows",
    "href": "rr_software.html#git-for-windows",
    "title": "Software for Reproducible Research with R",
    "section": "7.1 Git for Windows",
    "text": "7.1 Git for Windows\nYou can download it from either https://git-scm.com or https://gitforwindows.org. You can use this direct link for the Windows installer. Save that installation file to your compute, then run it."
  },
  {
    "objectID": "rr_software.html#git-for-macos",
    "href": "rr_software.html#git-for-macos",
    "title": "Software for Reproducible Research with R",
    "section": "7.2 Git for MacOS",
    "text": "7.2 Git for MacOS\nDownload instructions are available at\nhttps://git-scm.com/download/mac."
  },
  {
    "objectID": "posts/output/draft_vs_production.html",
    "href": "posts/output/draft_vs_production.html",
    "title": "Draft vs. Production Output",
    "section": "",
    "text": "This post is describes a practical example of applying the separation principle for reproducible research. Specifically, it describes why and how I separate draft versus production versions of my output. The post assumes you have already installed some specific software for reproducible research.\nI develop a research compendium for each of my statistical projects. My usual approach is to create a Git repository for a custom R package, part of which is a Quarto project. There are often multiple Quarto scripts that use R code chunks to do various tasks such as importing, cleaning, managing, and analyzing data, then reporting the results.\n\nWhy Separate Draft vs. Production Output?\nDeveloping my scripts is an iterative process. I render drafts many times per day as I add or edit code, but I only need to do occasional production runs. The draft outputs are disposable intermediate artifacts. If needed, I can recover interim draft output by returning to a specific Git commit and re-running the code as it existed at that point in time.\nProduction output files are another matter entirely. I find it useful to retain a version history of all rendered production output files distributed to clients, research partners, or other stakeholders. That way when someone wants to discuss an output with me, I can quickly find, open, and review it without having to re-run code.\n\n\nMy Requirements for Output Files\nFor most projects and scripts, I find it useful to impose a small set of requirements on output file names. They must:\n\nShow which script generated the output.\nClarify draft versus production status.\nDistinguish between different production runs of the same script.\nShow what output format was used.\n\n\n\n\n\n\n\nTip\n\n\n\nMy personal convention is to assign filenames by combining three pieces: a stem, a suffix, and a file extension.\n\nThe stem is a text string that matches the name of the script that produces the output (addressing requirement #1). For example, a script called Import_Data.qmd leads to a stem of Import_Data.\nThe suffix consists of either the string _Draft for draft outputs, or a date in _YYYY-MM-DD format for production outputs. That addresses both requirements #2 and #3.\nThe file extension then shows what output format was used (e.g., HTML or PDF). That addresses requirement #4.\n\nSo, Import_Data_Draft.html is a draft HTML output, while Import_Data_2025-01-16.html and Import_Data_2025-02-16.html are production HTML outputs run a month apart.\n\n\nDate-stamping the production output file names makes sorting files to find the most current version of the results simple. We can also easily compare those to prior versions because we retain all production outputs as separate files.\n\n\nDefaulting to Draft Output\nSaving every draft output file I generate would clutter my folders with tons of interim files that would differ in only small ways. Therefore, I just overwrite my draft output file each time I render a script on its own. That reduces storage space consumed. Storage space may be very cheap, but still ought to be used sensibly to facilitate usability and finding things.\nTo demonstrate my approach, I created a file called Example1.qmd that contains the following text.\n\n\n\n\n\n\nExample1.qmd\n\n\n\n---\ntitle: \"Example 1: Separating Draft vs. Production Output\"\nauthor: \n  - name: Steven J. Pierce\n    orcid: 0000-0002-0679-3019\n    email: pierces1@msu.edu\nformat: \n  html:\n    output-file: \"Example1_Draft.html\"\n    embed-resources: true \n  pdf:\n    output-file: \"Example1_Draft.pdf\"\n---\n\nThis file is used to demonstrate YAML configuration I use to set the default \noutput file names for draft HTML and PDF output. The same script can also \ngenerate production output by over-riding the default output file name at the \ntime of rendering. \n\n\n\nThe format: key section of the YAML header controls what types of output will be produced. This one specifies both html: and pdf: output files will be written and uses a separate output-file: key for each one to specify the default output file names. Rendering Example1.qmd on its own from within the RStudio interface (clicking the Render button) will create both files using the default names. Figure 1 shows the process at a conceptual level.\n\n\n\n\n\n\n\n\nGetDraft\n\n\n\nScript\n\nExample1.qmd\n(Quarto Script)\n\n\n\nHTML\n\nExample1_Draft.html\n(HTML Output)\n\n\n\nScript-&gt;HTML\n\n\nDefault\n\n\n\nPDF\n\nExample1_Draft.pdf\n(PDF Output)\n\n\n\nScript-&gt;PDF\n\n\nDefault\n\n\n\n\n\n\nFigure 1: Render Example1.qmd to Get Draft Output\n\n\n\n\n\nNaturally, you can remove one of the output format keys from the YAML header to get just one output file in whichever format you prefer.\nHere are links to the draft output files:\n\nExample1_Draft.html\nExample1_Draft.pdf\n\n\n\nObtaining Production Output\nWhat remains then is to demonstrate how I get those date-stamped production outputs. For that, I create a production script (e.g., Production_Run.qmd) that can render other scripts to automatically date-stamped, customized output file names as shown in Figure 2.\n\n\n\n\n\n\n\n\nGetProduction\n\n\n\nProd\n\nProduction_Run.qmd\n(Quarto Script)\n\n\n\nLog\n\nProduction_Run.html\n(HTML Output)\n\n\n\nProd-&gt;Log\n\n\nDefault\n\n\n\nScript\n\nExample1.qmd\n(Quarto Script)\n\n\n\nProd-&gt;Script\n\n\nPass\nfile\nnames\n\n\n\nHTML\n\nExample1_2025-02-16.html\n(HTML Output)\n\n\n\nScript-&gt;HTML\n\n\nCustomized\n\n\n\nPDF\n\nExample1_2025-02-16.pdf\n(PDF Output)\n\n\n\nScript-&gt;PDF\n\n\nCustomized\n\n\n\n\n\n\nFigure 2: Render Production_Run.qmd to Get Date-Stamped, Production Output\n\n\n\n\n\nBelow is the basic content of the Production_Run.qmd script. You would need to replace the double curly braces {r} with single curly braces {r} to get this to run (the double braces disable execution of those chunks).\nHere is a link to a production output file, namely Example1_2025-02-16.html\n\n\n\n\n\n\nProduction_Run.qmd\n\n\n\n---\ntitle: \"Production Run Control Script\"\nauthor: \n  - name: Steven J. Pierce\n    orcid: 0000-0002-0679-3019\n    email: pierces1@msu.edu\noutput-dir: examples\nformat: \n  html:\n    output-file: \"Production_Run.html\"\n    embed-resources: true \n---\n\nThis file is used to demonstrate using a production script to render other \nscripts to automatically date-stamped, customized file names.\n\n``` {{r}}\n#| label: load-packages\n\nlibrary(here)    # for here().\nlibrary(quarto)  # for quarto_render().\n```\n\nThe chunk below will render just the HTML version of the production output from \n`Example1.qmd`. \n\n``` {{r}}\n#| label: prod-render-Example1\n\nOutFile &lt;- paste0(\"Example1_\", Sys.Date(), \".html\")\n\n# Render the script in a fresh R session.\nquarto_render(input = here(\"examples/Example1.qmd\"), \n              output_format = \"html\",\n              execute_dir = here::here(\"examples\"),\n              output_file = OutFile)\n```"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Welcome my personal website! This is where I share materials on reproducible research, statistical consulting, and other topics I find interesting.\nSoftware for Reproducible Research with R\n\n\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        File Name\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\nHTML vs. PDF Output\n\n\n\nQuarto Tips\n\nReproducible Research\n\n\n\nChoosing between Quarto output formats.\n\n\n\n\n\n2025-05-13\n\n6 min\n\nhtml_vs_pdf.qmd\n\n\n\n\n\n\n\nAssessing Missing Data\n\n\n\nQuarto Tips\n\nMissing Data\n\nR\n\n\n\nExamples of Assessing Missing Data.\n\n\n\n\n\n2025-03-29\n\n16 min\n\nassess_missing_data.qmd\n\n\n\n\n\n\n\nDraft vs. Production Output\n\n\n\nQuarto Tips\n\nReproducible Research\n\nSeparation Principle\n\n\n\nHow I use Quarto to separate draft from production output files.\n\n\n\n\n\n2025-02-16\n\n6 min\n\ndraft_vs_production.qmd\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "examples/Example1_Draft.html",
    "href": "examples/Example1_Draft.html",
    "title": "Example 1: Separating Draft vs. Production Output",
    "section": "",
    "text": "This file is used to demonstrate YAML configuration I use to set the default output file names for draft HTML and PDF output. The same script can also generate production output by over-riding the default output file name at the time of rendering."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am an applied statistician and researcher at Michigan State University, working in the Center for Statistical Training and Consulting (CSTAT).\nSome of my current work focuses on:\n\nQuality improvement for children’s mental health health services.\nA cost-benefit analysis of EFNEP utilizing biomarkers of chronic disease risk.\n\nExamining the effect of a cognitive-behavioral therapy on psychological, functional, and disease-related outcomes in youth with childhood-onset lupus.\nPredicting the performance of medical students on a licensing examination."
  },
  {
    "objectID": "examples/Production_Run.html",
    "href": "examples/Production_Run.html",
    "title": "Production Run Control Script",
    "section": "",
    "text": "This file is used to demonstrate using a production script to render other scripts to automatically date-stamped, customized file names.\n\nlibrary(here)    # for here().\n\nhere() starts at C:/Users/pierces1/OneDrive - Michigan State University/CSTATRedirects/Documents/R/sjpierce.github.io\n\nlibrary(quarto)  # for quarto_render().\n\nThe chunk below will render just the HTML version of the production output from Example1.qmd.\n\nOutFile &lt;- paste0(\"Example1_\", Sys.Date(), \".html\")\n\n# Render the script in a fresh R session.\nquarto_render(input = here(\"examples/Example1.qmd\"), \n              output_format = \"html\",\n              execute_dir = here::here(\"examples\"),\n              output_file = OutFile)\n\npandoc --output Example1_2025-02-16.html\n  to: html\n  output-file: Example1_Draft.html\n  standalone: true\n  embed-resources: true\n  title-prefix: Piercing Insight\n  section-divs: true\n  html-math-method: mathjax\n  wrap: none\n  default-image-extension: png\n  css:\n    - ../styles.css\n  toc: true\n  \nmetadata\n  document-css: false\n  link-citations: true\n  date-format: long\n  lang: en\n  theme: spacelab\n  bibliography:\n    - ../references.bib\n  csl: ../apa.csl\n  title: 'Example 1: Separating Draft vs. Production Output'\n  author:\n    - name: Steven J. Pierce\n      orcid: 0000-0002-0679-3019\n      email: pierces1@msu.edu\n  \nOutput created: ..\\docs\\examples\\Example1_2025-02-16.html"
  },
  {
    "objectID": "packages.html",
    "href": "packages.html",
    "title": "R Packages",
    "section": "",
    "text": "In addition to providing access to custom R functions, one can use R packages as research compendiums (Marwick et al., 2018) that organize raw data, documentation, data management scripts, analysis scripts, and statistical results in a convenient format. The scripts can contain narrative text interleaved with R code that can manage data and insert formatted text, tables, figures, and reference sections when you render the script into a dynamic document. Below is a list of my publicly accessible R packages. I have others in preparation that will remain private repositories until relevant scholarly products have been published.\nSome of the packages listed below relied on R Markdown scripts, but I have subsequently adopted a newer, more capable set of software tools for producing the research compendiums I have in preparation now.\n\n1 piercer\nThe piercer package is my personal package of miscellaneous functions for use in my research and statistical consulting work. It is not particularly large, but developing it has been an excellent learning experience.\n\n\n2 SAWpaper\nThe SAWpaper package contains the code and raw output for one of my papers (Winke et al., 2023). This paper used a continuation-ratio model to examine the validity of a computer-adaptive self-assessment of second language learners’ speaking proficiency. The PDF files include all results reported in the paper, plus additional output that was omitted due to page limits.\n\n\n3 SSACHR\nThis SSACHR package contains the code and raw output for a paper that used criminal history data for suspected serial sexual offenders to examine the potential impact of mandatory forensic testing of sexual assault kits on crime prevention (Campbell et al., 2022). The PDF files therein include all results reported in the paper, plus additional output that was omitted due to page limits.\n\n\n4 References\n\n\nCampbell, R., Pierce, S. J., Goodman-Williams, R., & Feeney, H. (2022). A window of opportunity: Examining the potential impact of mandatory sexual assault kit (SAK) testing legislation on crime prevention. Psychology, Public Policy, and Law, 28(3), 446–458. https://doi.org/10.1037/law0000349 \n\n\nMarwick, B., Boettiger, C., & Mullen, L. (2018). Packaging data analytical work reproducibly using R (and friends). The American Statistician, 72(1), 80–88. https://doi.org/10.1080/00031305.2017.1375986\n\n\nWinke, P., Zhang, X., & Pierce, S. J. (2023). A closer look at a marginalized test method: Self-assessment as a measure of speaking proficiency. Studies in Second Language Acquisition, 45(2), 416–441. https://doi.org/10.1017/S0272263122000079"
  },
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Presentations",
    "section": "",
    "text": "This is where I’ll post information about recent or upcoming presentations\n\n1 CSTAT Webinars\nBelow is the abstract for my recent presentation (Pierce, 2025, October 2) called Reproducible research: Principles, practices, and tools for generating reproducible statistical analyses and reports. This was the second iteration of that presentation, with some expanded material compared to the previous one (Pierce, 2025, March 6). The most recent slides and example files are available in my CSTAT.RR2025v2 repository. A video recording is also available.\nAbstract\n\nThis seminar will introduce the audience to a set of principles, practices, and free, open-source software tools that enable scientists to generate reproducible statistical analyses and reports. We will cover why reproducibility is important, then offer a vision of how to enhance the reproducibility of your work, with concrete steps you can take to achieve that goal. We will discuss tailoring the degree of reproducibility you aim to achieve for a given project, which may vary due to project context or constraints. In terms of software, we will describe how R, RStudio, Quarto, and TinyTex comprise a powerful suite of tools that uses dynamic documents to automate producing fully-formatted reports, manuscripts, or slides complete with narrative text, analysis results, figures, tables, and references. Git and GitHub.com add further value through support for version control and collaboration on the source code for dynamic documents. The session will include conceptual content, examples of dynamic documents, and links to supporting resources the audience can use to accelerate learning how to make their work more reproducible.\n\n\n\n2 MSU Program Evaluation Occasional Speaker Series\nBelow is the abstract for my presentation (Pierce, 2024, December 5) called Fundamentals of missing data in evaluation. Slides are available in my FMDE2024 repository and a video recording is also available.\nAbstract\n\nThis talk will discuss some fundamental concepts and issues related to missing data in program evaluation contexts. We will cover why missing data matters plus types of missing data and how they affect statistical results. Then we will highlight how to describe the nature, scope, and patterns of missing data and how they relate to observed data. Finally, we will discuss some ways to prevent missing data and options for obtaining valid, unbiased statistical results even when there is missing data.\n\n\n\n3 Ann Arbor R Users’ Group\nBelow is the abstract for my presentation (Pierce, 2024, November 14) called R and Quarto: A foundation for generating reproducible reports. Slides and example files are available in my Pierce.AARUG2024 repository.\nAbstract\n\nThis presentation will discuss the importance of reproducibility and illustrate how R and Quarto are foundational pieces of an integrated set of tools for generating fully-formatted, reproducible reports in a variety of output formats. We’ll cover some key principles and practices that enhance reproducibility.\n\n\n\n4 American Evaluation Association 2024\nBelow is the abstract for my presentation (Pierce, 2024, October 21-26) called Generating reproducible statistical analyses and evaluation reports: Principles, practices, and free software tools. Slides and example files are available in my Pierce.AEA2024 repository.\nAbstract\n\nFully reproducible statistical analyses are ones for which investigators have shared all the materials required to exactly recreate their findings so others can verify them or conduct alternative analyses. That requires sharing the original (usually de-identified) data, supporting documentation, and the software code used to analyze the data. While reproducibility has been described as an attainable minimum standard for trustworthy, credible scientific work; it is not yet well-embedded in evaluators’ professional training. This session will introduce the audience to a set of principles, practices, and free, open-source software tools that enable evaluators to efficiently generate reproducible statistical analyses and evaluation reports. We will cover why reproducibility is important in an evaluation context, then offer a vision of how to improve the reproducibility of your work and suggest concrete steps you can take to achieve that goal. We will discuss tailoring the degree of reproducibility you aim to achieve for a given project, which may vary due to project context or constraints. In terms of software, we will describe how R, RStudio, Quarto, and TinyTex comprise a powerful suite of tools that can generate dynamic documents containing a mix of narrative text along with R code that can be compiled to automate producing a fully-formatted report, manuscript, or set of slides complete with narrative text, analysis results, figures, tables, and references. Git and GitHub.com add further value through support for version control and collaboration on the source code for dynamic documents. The session will include conceptual content, examples of dynamic documents, and links to supporting resources the audience can use to accelerate learning how to make their work more reproducible.\n\n\n\n5 References\n\n\nPierce, S. J. (2024, December 5). Fundamentals of missing data in evaluation [Invited oral presentation]. Program Evaluation Occasional Speaker Series hosted by Michigan State University Department of Psychology, East Lansing, MI, United States. https://github.com/sjpierce/FMDE2024\n\n\nPierce, S. J. (2024, November 14). R and Quarto: A foundation for generating reproducible reports [Invited oral presentation]. Ann Arbor R Users’ Group, Ann Arbor, MI, United States. https://github.com/sjpierce/Pierce.AARUG2024\n\n\nPierce, S. J. (2024, October 21-26). Generating reproducible statistical analyses and evaluation reports: Principles, practices, and free software tools [Demonstration session]. Evaluation 2024: Amplifying; Empowering Voices in Evaluation, the annual conference of the American Evaluation Association, Portland, OR, United States. https://github.com/sjpierce/Pierce.AEA2024\n\n\nPierce, S. J. (2025, March 6). Reproducible research: Principles, practices, and tools for generating reproducible statistical analyses and reports [Online seminar]. Center for Statistical Training and Consulting webinar series on Responsible and Ethical Conduct of Research. https://github.com/sjpierce/CSTAT.RR2025\n\n\nPierce, S. J. (2025, October 2). Reproducible research: Principles, practices, and tools for generating reproducible statistical analyses and reports [Online seminar]. Center for Statistical Training and Consulting webinar series on Responsible and Ethical Conduct of Research. https://github.com/sjpierce/CSTAT.RR2025v2"
  },
  {
    "objectID": "rr_software.html#rtools-for-windows-version-4.4-6459-6401-or-later",
    "href": "rr_software.html#rtools-for-windows-version-4.4-6459-6401-or-later",
    "title": "Software for Reproducible Research with R",
    "section": "4.1 RTools for Windows, version 4.4 (6459-6401 or later)",
    "text": "4.1 RTools for Windows, version 4.4 (6459-6401 or later)\nInstalling RTools is important because it contains some tools for compiling software packages and one of the R packages we will use later (the devtools package) depends on having RTools available. You can download RTools from CRAN. Either follow the CRAN links labeled Download R for Windows, then RTools, then RTools 4.4, then RTools44_installer, or use the direct link to the installer file. Save that installation file to your computer, then run it."
  },
  {
    "objectID": "posts/mda/assess_missing_data.html",
    "href": "posts/mda/assess_missing_data.html",
    "title": "Assessing Missing Data",
    "section": "",
    "text": "Examining the amount, nature, and patterns of missing data is crucial to properly handling missing data (McKnight et al., 2007; van Buuren, 2018; White et al., 2011). There are no hard and fast rules about how much missing data constitutes a serious problem, but more missing data usually means there is a greater risk of getting biased results if your analysis relies on listwise deletion to handle missing data. One has to make judgments about whether the amount and nature of the missing data issues in your study warrant the effort required to implement sophisticated solutions such as multiple imputation (MI) or full information maximum likelihood (FIML) estimation. It is best to make such judgments after a thorough assessment of the missingness. In short, you should diagnose the missing data situation before you decide whether and how to treat the problem.\nThis post discusses a few simple things one should do along the way and provides R code that I might use in a Quarto script to do them, along with commentary on the results obtained for an example data set.\nIn larger or more complex data sets this sort of assessment can be lengthy, so this post is by no means comprehensive. It’s just demonstrating some essential summaries that you should examine. For more extensive guidance on assessing missing data issues, see books by McKnight et al. (2007), Enders (2022), and Graham (2013).\n\n\n\n\n\n\nTip\n\n\n\nAll the example code operates on a data frame. Naturally, changing the data set fed into the code will change the results. Think carefully about which data set makes the most sense to use. You may need to subset a large data file by selecting the cases and variables relevant to your planned analyses before applying these techniques."
  },
  {
    "objectID": "posts/mda/assess_missing_data.html#amount-of-missing-data",
    "href": "posts/mda/assess_missing_data.html#amount-of-missing-data",
    "title": "Assessing Missing Data",
    "section": "4.1 Amount of Missing Data",
    "text": "4.1 Amount of Missing Data\nYou should always examine the dimensions of your data set in terms of numbers of cases (rows), variables (columns), and values (rows x columns). Extracting those values from a data set is easy using functions such as nrow() and ncol() as shown below. The number of values is obtained by multiplying the numbers of cases and variables. This sets the context for examining how much of the data is missing. Additional functions from the mice package make it easy to extract numbers of complete cases (ncc()) and incomplete cases nic(). Creative combined use of the sum(), colSums(), and is.na() functions can get us similar counts of the number of complete and incomplete variables and of the number of complete (non-missing) and incomplete (missing) values.\nThe chunk below generates Table 1, which shows a convenient summary of all those counts. I plan to eventually add functions to my piercer package to automate generating this sort of summary if I can’t find an existing package that has one I like. That would reduce the amount of code required to get the table.\n\n\nCode\n```{r}\n#| label: tbl-dataset-info\n#| tbl-cap: Numbers of Complete and Incomplete Cases, Variables, and Values\n\nN_cases  &lt;- nrow(boys)                     # All cases\nN_ccases &lt;- ncc(boys)                      # Complete cases\nN_icases &lt;- nic(boys)                      # Incomplete cases  \nN_vars   &lt;- ncol(boys)                     # All variables\nN_cvars  &lt;- sum(colSums(is.na(boys)) == 0) # Complete variables\nN_ivars  &lt;- sum(colSums(is.na(boys)) &gt; 0)  # Incomplete variables\nN_vals   &lt;- N_cases*N_vars                 # All values\nN_cvals  &lt;- sum(!is.na(boys))              # Complete values (non-missing)\nN_ivals  &lt;- sum(is.na(boys))               # Incomplete values (missing)\n\ntibble(Subset = c(\"Complete\", \"Incomplete\", \"All\"),\n       Cases = c(N_ccases, N_icases, N_cases),\n       Cases_P = 100*Cases/N_cases, \n       Variables = c(N_cvars, N_ivars, N_vars),\n       Variables_P = 100*Variables/N_vars,\n       Values = c(N_cvals, N_ivals, N_vals),\n       Values_P = 100*Values/N_vals) %&gt;% \n  kable(., format = \"html\", digits = 1, \n        col.names = c(\"Subset\", rep(c(\"n\", \"%\"), times = 3))) %&gt;%\n  kable_styling() %&gt;% \n  add_header_above(., \n                   header = c(\" \", \"Cases\" = 2, \"Variables\" = 2, \"Values\" =  2))\n```\n\n\n\n\nTable 1: Numbers of Complete and Incomplete Cases, Variables, and Values\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCases\n\n\nVariables\n\n\nValues\n\n\n\nSubset\nn\n%\nn\n%\nn\n%\n\n\n\n\nComplete\n223\n29.8\n1\n11.1\n5110\n75.9\n\n\nIncomplete\n525\n70.2\n8\n88.9\n1622\n24.1\n\n\nAll\n748\n100.0\n9\n100.0\n6732\n100.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nA value is just the datum for a specific case on a specific variable. It is complete (observed) when it non-missing, and incomplete when it is missing.\nA case is complete when all values on the row (across variables) are complete; it becomes incomplete if any variable in that row has a missing value.\nSimilarly, a variable is complete when all values in that column (across cases) are complete but it becomes incomplete if any case in that column has a missing value.\n\n\n\nHere, we can see that there’s a reasonably large sample size (748) but there are only 9 variables. That yields a total of 6732 values in the data set.\nWe see different percentages of completeness depending on whether we look at cases, variables, or values. This is a cross-sectional dataset, so we don’t have to consider a time dimension but if we had a longitudinal study we might need to examine that as an additional dimension in the assessment. Similarly, this is not a multilevel study design but if it were, we might need to examine missingness at each level of the design.\nThe percent of incomplete cases is staggeringly high (70.2%), as is the percent of incomplete variables (88.9%). However, we can also see that the percent of missing values in the full data frame is much lower (24.1%), though it may still be large enough to cause some concern. We need to know more before we make decisions."
  },
  {
    "objectID": "posts/mda/assess_missing_data.html#univariate-missingness",
    "href": "posts/mda/assess_missing_data.html#univariate-missingness",
    "title": "Assessing Missing Data",
    "section": "4.2 Univariate Missingness",
    "text": "4.2 Univariate Missingness\nWhile Table 1 is a useful overview of the amount of missing data, we want to do a more granular examination as well. For example, we now know that there are 8 incomplete variables but the amount of missing data in each of those variables could be quite different. Maybe there’s only one missing value among hundreds of cases for one variable, but hundreds of missing values for another variable. That could be very important.\nSo, the next step is to examine univariate summaries of the amount of missing data for each variable. The chunk below constructs Table 2, which shows such a summary.\n\n\nCode\n```{r}\n#| label: tbl-missing-by-var\n#| tbl-cap: !expr paste0(\"Univariate Missingness in Boys Growth Data, (N = \", \n#|                       nrow(boys), \")\")\n\nboys %&gt;% \n  list() %&gt;%\n  map_dfr(~ tibble(Name = names(.x),\n                   N_Valid = colSums(!is.na(.x)),\n                   N_Missing = colSums(is.na(.x)),\n                   Pct_Missing = 100*(N_Missing/nrow(boys)))) %&gt;%\n  rowid_to_column(., \"Position\") %&gt;%\n  kable(., format = \"html\", digits = 1) %&gt;%\n  kable_styling() \n```\n\n\n\n\nTable 2: Univariate Missingness in Boys Growth Data, (N = 748)\n\n\n\n\n\n\nPosition\nName\nN_Valid\nN_Missing\nPct_Missing\n\n\n\n\n1\nage\n748\n0\n0.0\n\n\n2\nhgt\n728\n20\n2.7\n\n\n3\nwgt\n744\n4\n0.5\n\n\n4\nbmi\n727\n21\n2.8\n\n\n5\nhc\n702\n46\n6.1\n\n\n6\ngen\n245\n503\n67.2\n\n\n7\nphb\n245\n503\n67.2\n\n\n8\ntv\n226\n522\n69.8\n\n\n9\nreg\n745\n3\n0.4\n\n\n\n\n\n\n\n\n\n\nInspection of Table 2 shows that age is the only variable with no missing data. There is only a small amount of missing data for basic anthropometric measurements (hgt, wgt, bmi, hc) and region (reg), but there is a large amount of missing data for the three measures of pubertal development (gen, phb, and tv).\nThe maximum percent missing observed in Table 2 for a single variable can never be lower than the overall percent of incomplete cases from Table 1. However, the percent of incomplete cases can easily be higher than any of the percentages for individual variables because missing data may occur in different variables on different cases.\nIf our planned analyses only required the anthropometric variables and region, this data might be considered to have a fairly small amount of missing data. One could remove the irrelevant variables from the data set and then re-create Table 1 to get better estimates of how many incomplete cases would be dropped by listwise deletion. On the other hand, analyses requiring any of the pubertal development measures would require solving an extremely large missing data problem."
  },
  {
    "objectID": "posts/mda/assess_missing_data.html#multivariate-patterns-of-missingness",
    "href": "posts/mda/assess_missing_data.html#multivariate-patterns-of-missingness",
    "title": "Assessing Missing Data",
    "section": "4.3 Multivariate Patterns of Missingness",
    "text": "4.3 Multivariate Patterns of Missingness\nAfter examining the univariate missingness, we need to shift our focus to considering the patterns of missing data across variables. These multivariate patterns of missingness can be quite useful in better understanding that nature of the missingness issues in your data. Figure 1 illustrates what I mean by missing data patterns. It is generated by the ggmice function plot_pattern(), which is a cleaner, better labeled alternative to the graphical output from the mice function md.pattern().\n\n\nCode\n```{r}\n#| label: fig-patterns\n#| fig-cap: \"Missingness Patterns for Dutch Boys Growth Study Data (748 boys, 9 \n#|           variables, 1 time point) [@Fredriks-RN8696]\"\n\nplot_pattern(boys, square = FALSE, rotate = FALSE)\n```\n\n\n\n\n\n\n\n\nFigure 1: Missingness Patterns for Dutch Boys Growth Study Data (748 boys, 9 variables, 1 time point) (Fredriks et al., 2000)\n\n\n\n\n\nEach row in the graph represents a distinct pattern of observed versus missing values across the whole set of variables. The left side is annotated with the number of cases (i.e., boys) showing each pattern, the right side tells you how many missing values there are on that row, and the bottom shows the total number of missing values for each variable. The variables are sorted from left to right in ascending order by number of missing values, so those on the right have the most missing data. The annotation below the color coding legend shows the total number of missing values. Some of the information in this figure overlaps with that in the tables above, but I find it useful to have the percentages in those tables along with the counts shown here.\nThe first row of Figure 1 shows the 223 boys have complete data. The rest of the plot shows that this data set has a lot of holes in it. For example, we can see that the most common pattern is that 437 boys (58.4%) have missing data for all 3 pubertal development measures but observed data on all the other measures. We can also see a number of patterns exhibited by only one boy each. The patterns with the highest frequencies are probably the most useful ones to look at in this type of plot. Beware that data sets containing both large samples and large numbers of variables can make plots like this hard to read. You may have to get creative about using these tools with subsets of the data in such cases.\nAnother thing we can see here is that body mass index is always missing whenever either height or weight is missing. That’s construct missingness caused by item missingness because BMI is a measure derived from both the height and weight variables. Solving any missing data for height and weight appropriately will let you solve missing BMI data as well. The mice package has features designed to handle preserving such deterministic relationships between variables during imputation.\nInspect the patterns of missingness and try to make sense of them. That can reveal things like contiguous blocks of variables that are always missing together, which might be a result of things like participants skipping an entire section of a survey. In a longitudinal study where the data file is laid out in wide, multivariate format (repeated measures over time are represented by sets of columns), wave non-response or dropout might show up the same way."
  },
  {
    "objectID": "posts/mda/assess_missing_data.html#predictors-of-missingness",
    "href": "posts/mda/assess_missing_data.html#predictors-of-missingness",
    "title": "Assessing Missing Data",
    "section": "4.4 Predictors of Missingness",
    "text": "4.4 Predictors of Missingness\nIdentifying predictors of missingness is another step in assessing the nature of the missing data. This stage of assessing missing data issues can become quite extensive if there are many variables. Simple bivariate analyses may suffice, but you can use multivariate methods too. Just make sure you use models appropriate to the types of variables (nominal, ordinal, interval, ratio, counts, etc.) being examined in any given analysis.\nObserved variables that are associated with whether another variable is missing reveal that your data are incompatible with the missing completely at random (MCAR) mechanism; instead the mechanism must either be missing at random (MAR) or missing not at random (MNAR).\nSuppose in this boys growth study, we wanted to check whether the pubertal development variables were more likely to be missing for the youngest boys. That might happen if data collection procedures specified assessing the pubertal variables only for boys above some threshold age. We could create a set of binary missingness indicators, one for each pubertal variable, coded 0 if the pubertal variable was missing and 1 if it was observed. Then a logistic regression model could test whether age predicts the missingness indicator well. Table 3 shows that the missingness of the tv variable decreases with each additional year in age. This is actionable information about a potential source of bias if missing data are not handled properly.\n\n\nCode\n```{r}\n#| label: tbl-m1\n#| tbl-cap: Logistic Regression Model Predicting Missingness of tv\n#| warning: false\n\nm1 &lt;- glm(is.na(tv) ~ age, family = binomial, data = boys)\noptions(modelsummary_factory_html = 'kableExtra')\nmodelsummary(m1, exponentiate = TRUE, \n             output = \"kableExtra\", align = \"lrrrrr\", escape = FALSE,\n             col.names = c(\"Term\", \"OR\", \"OR.LL\", \"OR.UL\", \"t\", \"p\"),\n             shape = term ~ model + statistic,\n             gof_omit = \"BIC|AIC\",\n             statistic = c(\"conf.int\", \"statistic\", \"p.value\"))\n```\n\n\n\n\nTable 3: Logistic Regression Model Predicting Missingness of tv\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(1)\n\n\n\nTerm\nOR\nOR.LL\nOR.UL\nt\np\n\n\n\n\n(Intercept)\n16.934\n11.230\n26.545\n12.921\n&lt;0.001\n\n\nage\n0.832\n0.806\n0.858\n−11.441\n&lt;0.001\n\n\nNum.Obs.\n748\n\n\n\n\n\n\nLog.Lik.\n−Inf\n\n\n\n\n\n\nF\n130.902\n\n\n\n\n\n\nRMSE\n0.42\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnother thought for this example data is that maybe the rate of missingness for tv varies across regions. We can check that with a contingency table and a chi-square test. It does not look like region is associated with missingess of tv in these data.\n\n\nCode\n```{r}\n#| label: tbl-reg-tv\n#| tbl-cap: Contingency Table Region by Missingness of tv\n\nxtabs(~reg + is.na(tv), data = boys) %&gt;% \n  kable(col.names = c(\"Region\", \"FALSE (n)\", \"TRUE (n)\")) %&gt;% \n  kable_styling() %&gt;% \n  add_header_above(., header = c(\" \", \"Missing tv\" = 2))\n\nxtabs(~reg + is.na(tv), data = boys) %&gt;% chisq.test()\n```\n\n\n\n    Pearson's Chi-squared test\n\ndata:  .\nX-squared = 6.1038, df = 4, p-value = 0.1915\n\n\n\n\nTable 4: Contingency Table Region by Missingness of tv\n\n\n\n\n\n\n\n\n\n\n\n\n\nMissing tv\n\n\n\nRegion\nFALSE (n)\nTRUE (n)\n\n\n\n\nnorth\n32\n49\n\n\neast\n52\n109\n\n\nwest\n62\n177\n\n\nsouth\n60\n131\n\n\ncity\n20\n53\n\n\n\n\n\n\n\n\n\n\nPut some thought into what to examine and how to do it. You may not be able to examine everything possible in a given data set, so be judicious about looking for potential predictors of missingness. Use substantive knowledge to guide those decisions if possible."
  },
  {
    "objectID": "posts/mda/assess_missing_data.html#level-2-data-set",
    "href": "posts/mda/assess_missing_data.html#level-2-data-set",
    "title": "Assessing Missing Data",
    "section": "5.1 Level 2 Data Set",
    "text": "5.1 Level 2 Data Set\nStart by using a level 2 data set containing one row per school. Apply the techniques discussed above to understand missingness for school-level measures. How many schools have complete versus incomplete data? Missing school-level measures will affect all students from those schools after we merge the two levels of data."
  },
  {
    "objectID": "posts/mda/assess_missing_data.html#level-1-data-set",
    "href": "posts/mda/assess_missing_data.html#level-1-data-set",
    "title": "Assessing Missing Data",
    "section": "5.2 Level 1 Data Set",
    "text": "5.2 Level 1 Data Set\nThen, use a level 1 data set containing one row per student. This data set should only contain student-level measures so you can get a pure look at how student-level missing data issues might affect your analysis."
  },
  {
    "objectID": "posts/mda/assess_missing_data.html#combined-dataset",
    "href": "posts/mda/assess_missing_data.html#combined-dataset",
    "title": "Assessing Missing Data",
    "section": "5.3 Combined Dataset",
    "text": "5.3 Combined Dataset\nMerge the level 2 measures into the level 1 data set to get a combined data set with one row per student that contains both student- and school-level measures. Repeat the missing data analysis on this combined data to learn how much the intersection of missingness across levels affects the number of complete cases. Comparing the number of complete cases between the combined data set and the level 1 data set will reveal how much missing school-level measures will exacerbate any issues observed in the level 1 data. Some students may have complete data only when considering student-level measures, but have incomplete data after merging in school-level measures. That will have consequences for your analysis."
  },
  {
    "objectID": "rr_software.html#rtools-for-windows-version-4.5-6536-6492-or-later",
    "href": "rr_software.html#rtools-for-windows-version-4.5-6536-6492-or-later",
    "title": "Software for Reproducible Research with R",
    "section": "4.1 RTools for Windows, version 4.5 (6536-6492 or later)",
    "text": "4.1 RTools for Windows, version 4.5 (6536-6492 or later)\nInstalling RTools is important because it contains some tools for compiling software packages and one of the R packages we will use later (the devtools package) depends on having RTools available. You can download RTools from CRAN. Either follow the CRAN links labeled Download R for Windows, then RTools, then RTools 4.5, then RTools44_installer, or use the direct link to the installer file. Save that installation file to your computer, then run it."
  },
  {
    "objectID": "posts/output/html_vs_pdf.html",
    "href": "posts/output/html_vs_pdf.html",
    "title": "HTML vs. PDF Output",
    "section": "",
    "text": "I often need to share statistical output and reports with my colleagues and research partners. These outputs are typically part of a research compendium designed to enhance reproducibility. Quarto is one of my preferred software tools in part because it can render dynamic documents into several different output file formats. This post explains a few thoughts about how I choose which output file format to use. The formats I use most often include HTML, PDF, and Revealjs.\nA few things to consider when choosing an output format are:\n\nUsage. Is the output going to be used as a document designed for reading (by either myself or someone else), or slides that will be used in a presentation?\nMedium. Do I want to optimize for reading the output on a computer screen or from a printed hard copy? How well does the output format support the intended medium?\nCode Display. How does the format support showing versus hiding the code used to manage data and produce output elements like tables and graphs?\nNavigation. For longer documents, how does the format support navigation between sections or searching for specific strings of text?\nAnnotation. Do we need to be able to annotate the output easily? Some partners send me feedback by annotating an output file and sending it back, or annotate the output for their own use.\nBranding. How does the output format support formatting output to achieve consistent branding via logos, color palettes, fonts, and other design elements?\n\n\nHTML\nIn terms of usage, default HTML output can be effective for documents, but a specialized variant of HTML output called Revealjs is better for producing presentation slides. Quarto supports Revealjs quite well. It can also write out PowerPoint slides, but supports more capabilities with Revealjs so one has more control over how the slides look and work in Revealjs than in PowerPoint.\nHTML is good for on-screen reading. Most popular web browsers are capable of displaying it well and adjusting for things like screen resolution and size. While one can print HTML output with a web browser, it is hard to ensure that the print pagination and layout for HTML output will be attractive and sensible.\nOne thing I like about HTML output is the control you have over code display. I particularly like to echo the code (echo: true) with code folding enabled (code-fold: true). That inserts buttons in the output that dynamically show individual blocks of code on demand, but default to collapsing them, allowing you to focus more on narrative text, tables, and figures without sacrificing the reproducibility advantages of being able to see the code in the same document. You can also configure controls that hide or reveal all the code simultaneously, make it easy to copy the code.\nThe HTML format also supports adding a floating, hyperlinked table of contents (TOC) on the side that remains visible regardless of what part of the document you are currently viewing. Clicking a topic in the TOC takes you to the relevant section. That’s very convenient for navigating long documents. You can control how many layers of section headings are displayed in the TOC. Quarto cross-references offer a way to embed other navigation hyperlinks in the output. Searching for a string of text within an HTML output file is easy because browsers usually have built-in search tools.\nOne disadvantage is that HTML does not provide an easy way for people to annotate the output file.\nBranding can be customized via by creating a _brand.yml, supplying logo files, picking an existing HTML theme, supplying a CSS file, or supplying a Sass file. You may need to experiment a bit to figure out which combination of those elements to use to get the desired results.\n\n\nPDF\nQuarto produces PDF output by using pandoc to convert your output into a LaTeX file (*.tex) which is then converted to its final PDF format. LaTeX is a sophisticated typesetting system that has been around for about 40 years now. It is really good for creating documents designed to be read. Beamer, a LaTeX package that facilitates creating PDF presentation slides, is supported by Quarto. Personally, I prefer Revealjs over Beamer for making slides, but Beamer is a well established tool used by many people.\nThe PDF format is fantastic for making documents you want to print. You can control every aspect of layout, formatting, pagination, and so on. The level of professional polish is limited only by your programming and design skills. Fortunately, you can also read PDF files on computer screens very easily. I choose PDF when I need something that must print well but may also be read on-screen.\nWith respect to code display, PDF is less flexible than HTML output. Showing the code for reproducibility purposes makes the document longer because chunks of code are interleaved among the narrative text, tables, and figures and cannot be collapsed. That can impair reading flow. One has to either show the code, entirely omit it, or produce two alternate versions of the output files (one showing the code and the other omitting it). I have used the latter approach on several projects, usually by creating a run script that serves as a detailed reproducibility log and does the main computations, then saves out selected R objects that will be used by a deliver script, which produces a report designed for delivery to other people. That latter output will omit the code chunks and emphasize clear, concise, communication of the methods and results.\nAs with HTML format, you can add a hyperlinked TOC to PDF output, control how many layers of section headings are displayed in it, and use cross-references to embed other navigation hyperlinks in the output. The TOC shows up at the start of the PDF file, below the title and author information. Depending on your PDF reader software, you can also view the TOC in a sidebar. As with HTML output, clicking a topic in the TOC takes you to the relevant section and facilitates navigating long documents. Finally, PDF readers usually have robust search tools for locating strings of text within a document.\nUsers who want to highlight or annotate parts of a PDF output file can do so easily. Annotation features are available in most PDF reading software.\nThe Quarto branding tools used for HTML don’t yet seem to support PDF output. Quarto’s support for inserting raw LaTeX code gives you access to tons of tools for controlling output typesetting, layout, and branding. You can customize fonts and colors, insert logos, control figure and table placement, add page breaks, and more. That may sometimes require digging into the nuances of LaTeX to get specific formatting. That can be a bit challenging at times but web searches, combined with some patience and tinkering can yield good solutions."
  },
  {
    "objectID": "rr_software.html#rtools-for-windows-version-4.5-6608-6492-or-later",
    "href": "rr_software.html#rtools-for-windows-version-4.5-6608-6492-or-later",
    "title": "Software for Reproducible Research with R",
    "section": "4.1 RTools for Windows, version 4.5 (6608-6492 or later)",
    "text": "4.1 RTools for Windows, version 4.5 (6608-6492 or later)\nInstalling RTools is important because it contains some tools for compiling software packages and one of the R packages we will use later (the devtools package) depends on having RTools available. You can download RTools from CRAN. Either follow the CRAN links labeled Download R for Windows, then RTools, then RTools 4.5, then RTools44_installer, or use the direct link to the installer file. Save that installation file to your computer, then run it."
  },
  {
    "objectID": "rr_software.html#rtools-for-windows-version-4.5-6691-6492-or-later",
    "href": "rr_software.html#rtools-for-windows-version-4.5-6691-6492-or-later",
    "title": "Software for Reproducible Research with R",
    "section": "4.1 RTools for Windows, version 4.5 (6691-6492 or later)",
    "text": "4.1 RTools for Windows, version 4.5 (6691-6492 or later)\nInstalling RTools is important because it contains some tools for compiling software packages and one of the R packages we will use later (the devtools package) depends on having RTools available. You can download RTools from CRAN. Either follow the CRAN links labeled Download R for Windows, then RTools, then RTools 4.5, then RTools44_installer, or use the direct link to the installer file. Save that installation file to your computer, then run it."
  }
]